{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/netspractice/advanced_gnn/blob/main/lab05_deep_generaion/lab.ipynb","timestamp":1686156573444}],"collapsed_sections":["mDA79edgioG2","Ofp7exBaGmIl","AlUJb8DmG-aJ"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"nhK0LDxTGaND"},"source":["# Lab — Deep Graph Generative Models"]},{"cell_type":"markdown","source":["In this lab we consider deep graph generation model on synthetic dataset."],"metadata":{"id":"-qUGgwa6CVTx"}},{"cell_type":"markdown","metadata":{"id":"mDA79edgioG2"},"source":["### Mini graph classification dataset"]},{"cell_type":"markdown","source":["First, define the custom dataset based on DGL MiniGCDataset."],"metadata":{"id":"lk2uTaOIChD8"}},{"cell_type":"code","metadata":{"id":"00BSVPv7j4pH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f9762657-1909-4660-ab3f-c1ca45e55d1c","executionInfo":{"status":"ok","timestamp":1686156564033,"user_tz":-180,"elapsed":8239,"user":{"displayName":"","userId":""}}},"source":["#!pip install dgl-cu111 -f https://data.dgl.ai/wheels/repo.html -q\n","!pip install dgl -q"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/5.9 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.8/5.9 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","metadata":{"id":"g_nk7-p8kGC2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e085b181-e599-45ec-e0ba-81fc8da77ef6","executionInfo":{"status":"ok","timestamp":1686156572910,"user_tz":-180,"elapsed":8899,"user":{"displayName":"","userId":""}}},"source":["import torch\n","from torch import nn\n","from torch.utils.data import Subset, DataLoader\n","import torch.nn.functional as F\n","from torch.optim import Adam, SGD, RMSprop\n","\n","import dgl\n","from dgl.data import DGLDataset, MiniGCDataset\n","from dgl.dataloading import GraphDataLoader\n","from dgl.nn import GraphConv, AvgPooling\n","\n","import networkx as nx\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.manifold import TSNE\n","from sklearn.decomposition import PCA\n","from tqdm.notebook import tqdm\n","\n","from IPython.display import clear_output"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["DGL backend not selected or invalid.  Assuming PyTorch for now.\n"]},{"output_type":"stream","name":"stdout","text":["Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"]}]},{"cell_type":"code","metadata":{"id":"Kat9t8CmjAsv"},"source":["class OrderingGCD(MiniGCDataset):\n","    def process(self):\n","        self.full_adj = []\n","        super(OrderingGCD, self).process()\n","        for graph in tqdm(self.graphs):\n","            G1 = nx.Graph(graph.to_networkx())\n","            G2 = graph.to_networkx()\n","\n","            degree = list(nx.degree_centrality(G1).values())\n","            betweenness = list(nx.betweenness_centrality(G1).values())\n","            closeness = list(nx.closeness_centrality(G1).values())\n","            eigenvector = list(nx.eigenvector_centrality(G1, max_iter=200).values())\n","            node_feat = torch.FloatTensor(\n","                np.vstack([degree, betweenness, closeness, eigenvector]).T\n","            )\n","            graph.ndata['feat'] = node_feat\n","\n","            adj = graph.adj().to_dense()\n","            top_degree = adj.sum(dim=1).argmax()\n","            bfs_seq = dgl.bfs_nodes_generator(graph, top_degree)\n","            idx = torch.cat(bfs_seq)\n","            adj = adj[idx, :][:, idx]\n","            k = self.max_num_v - adj.shape[0]\n","            full_adj = F.pad(adj, (0, k, 0, k))\n","            full_adj = full_adj.reshape(1, self.max_num_v, self.max_num_v)\n","\n","            self.full_adj.append(full_adj)\n","\n","    def __getitem__(self, idx):\n","        return self.graphs[idx], self.labels[idx], self.full_adj[idx]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pqm6yyx5k3Yc"},"source":["graph_type = {\n","    0: 'cycle graph',\n","    1: 'star graph',\n","    2: 'wheel graph',\n","    3: 'lollipop graph',\n","    4: 'hypercube graph',\n","    5: 'grid graph',\n","    6: 'clique graph',\n","    7: 'circular ladder graph'\n","}\n","\n","max_num_v = 12\n","dataset = OrderingGCD(\n","    num_graphs=800*2,\n","    min_num_v=6,\n","    max_num_v=max_num_v,\n","    seed=0,\n","    force_reload=True\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let us look at some graphs from the dataset."],"metadata":{"id":"CwWC9y4NCuTi"}},{"cell_type":"code","metadata":{"id":"uqnqBpMCHks-"},"source":["N = len(dataset)\n","plt.figure(figsize=(10, 10))\n","np.random.seed(0)\n","for i in range(16):\n","    plt.subplot(4, 4, i+1)\n","    g, l, adj = dataset[np.random.randint(N)]\n","    g = nx.Graph(g.to_networkx())\n","    g.remove_edges_from(nx.selfloop_edges(g))\n","    nx.draw_kamada_kawai(g, node_size=30, node_color=[plt.cm.Set1.colors[l.item()]])\n","    plt.title(graph_type[l.item()])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ofp7exBaGmIl"},"source":["### Graph VAE"]},{"cell_type":"markdown","source":["Consider the graph generation method based on Variational Autoencoder that can be used for generaion small graphs."],"metadata":{"id":"1WTdQaJKC1gP"}},{"cell_type":"markdown","metadata":{"id":"cKDY9RWqG1uh"},"source":[" Simonovsky and Komodakis [2018] https://arxiv.org/abs/1802.03480"]},{"cell_type":"markdown","metadata":{"id":"gb754nB7VkJV"},"source":["<img src='https://raw.githubusercontent.com/netspractice/advanced_gnn/made2021/lab_deep_generation/graph_vae.png' width=800>"]},{"cell_type":"markdown","metadata":{"id":"lfnKGnYlNjgI"},"source":["GCN encoder with graph pooling is used for mapping graph into the N(0, I) latent space."]},{"cell_type":"code","metadata":{"id":"Njn3S3toifuw"},"source":["class GaussianGCN(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super().__init__()\n","        self.conv = GraphConv(input_dim, hidden_dim)\n","        self.mu = GraphConv(hidden_dim, output_dim)\n","        self.sigma = GraphConv(hidden_dim, output_dim)\n","        self.pooling = AvgPooling()\n","\n","    def forward(self, g, features):\n","        h = self.conv(g, features)\n","        h = F.relu(h)\n","        mu = self.mu(g, h)\n","        sigma = F.softplus(self.sigma(g, h))\n","        return self.pooling(g, mu), self.pooling(g, sigma)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ZocSQX2Wuyv"},"source":["graph, label, adj = dataset[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ou3NXvbbCAXb"},"source":["encoder = GaussianGCN(input_dim=4, hidden_dim=256, output_dim=128)\n","z_mu, z_sigma = encoder(graph, graph.ndata['feat'])\n","z_mu.shape, z_sigma.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uxa9sy3JNoTo"},"source":["Probabilistic fully-connected graph decoder is used for reconstraction of the adjacency matrix."]},{"cell_type":"code","metadata":{"id":"rowrR9RxBZlq"},"source":["class MLP(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, n_nodes):\n","        super().__init__()\n","        self.n_nodes = n_nodes\n","        self.MLP = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, n_nodes**2),\n","            nn.Sigmoid()\n","        )\n","    def forward(self, x):\n","        x = self.MLP(x)\n","        return x.reshape(-1, self.n_nodes, self.n_nodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6LwvXAXGJz4W"},"source":["decoder = MLP(input_dim=128, hidden_dim=256, n_nodes=max_num_v)\n","pred_adj = decoder(z_mu)\n","pred_adj = pred_adj[0]\n","pred_adj.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Negative ELBO (evidence lower bound) is optimized during VAE training."],"metadata":{"id":"E_Irmh1eDgnV"}},{"cell_type":"code","metadata":{"id":"cFB4y8imL2BI"},"source":["def neg_elbo(pred_adj, adj, sigma, mu, batch_size):\n","    likelihood = -F.binary_cross_entropy(pred_adj, adj)\n","    d_kl = -(1 + torch.log(sigma**2) - mu**2 - sigma**2) * 0.5 / batch_size\n","    d_kl = d_kl.sum(1).mean()\n","    elbo = likelihood - d_kl\n","    return -elbo"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Prepare the dataloader for the VAE training."],"metadata":{"id":"VdSMtZaqD1Vo"}},{"cell_type":"code","metadata":{"id":"5ejYhYPrOzI4"},"source":["def collate(sample):\n","    graphs, labels, full_adj = map(list, zip(*sample))\n","    graph = dgl.batch(graphs)\n","    labels = torch.tensor(labels)\n","    full_adj = torch.cat(full_adj)\n","    return graph, labels, full_adj\n","\n","batch_size = 32\n","train_dataloader = DataLoader(\n","    dataset, batch_size=batch_size, drop_last=True, collate_fn=collate, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T1qBKKoKPlap"},"source":["Define training method."]},{"cell_type":"code","metadata":{"id":"VHPWrwv-PnNQ"},"source":["def train(encoder, decoder, dataloader, opt, batch_size, emb_dim):\n","    encoder.train()\n","    decoder.train()\n","    _train_loss = []\n","    for graph, label, full_adj in dataloader:\n","        mu, sigma = encoder(graph, graph.ndata['feat'])\n","        z = torch.randn(batch_size, emb_dim)\n","        z = z * sigma + mu\n","        pred_adj = decoder(z)\n","        loss = neg_elbo(pred_adj, full_adj, sigma, mu, batch_size)\n","\n","        opt.zero_grad()\n","        loss.backward()\n","        opt.step()\n","        _train_loss.append(loss.item())\n","    return sum(_train_loss) / len(_train_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define models and an optimizer."],"metadata":{"id":"A2w8qQKMEdr0"}},{"cell_type":"code","metadata":{"id":"qe74_WGoRrrG"},"source":["emb_dim = 32\n","encoder = GaussianGCN(input_dim=4, hidden_dim=256, output_dim=emb_dim)\n","decoder = MLP(input_dim=emb_dim, hidden_dim=256, n_nodes=max_num_v)\n","params = list(encoder.parameters()) + list(decoder.parameters())\n","opt = Adam(params, lr=0.005)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WcHyAkZwMx9O"},"source":["n_epochs = 100\n","train_loss = []\n","for epoch in range(n_epochs):\n","    loss = train(encoder, decoder, train_dataloader, opt, batch_size, emb_dim)\n","    train_loss.append(loss)\n","    plt.plot(train_loss[-50:], label='train')\n","    plt.legend()\n","    plt.title('Negative ELBO loss. Epoch: {}/{}'.format(epoch+1, n_epochs))\n","    plt.show()\n","    clear_output(wait=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GGX9w9fVYmN7"},"source":["Let us check the graph generation. We will descretize an adjacency matrix and drop zero colums and rows."]},{"cell_type":"code","metadata":{"id":"8AfxdM6dY--L"},"source":["def discrete_adj(batch):\n","    adjs = []\n","    for pred_adj in batch:\n","        _adj = (pred_adj > 0.5).type(torch.LongTensor)\n","        _adj[range(max_num_v), range(max_num_v)] = 0\n","        mask = (_adj.sum(dim=1) != 0) | (_adj.sum(dim=0) != 0)\n","        adjs.append(_adj[mask, :][:, mask])\n","    return adjs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3wjYEUK5YJxx"},"source":["plt.figure(figsize=(10, 10))\n","z = torch.randn(16, emb_dim)\n","decoder.eval()\n","with torch.no_grad():\n","    pred_adj = decoder(z)\n","gen_adj = discrete_adj(pred_adj)\n","for i in range(16):\n","    plt.subplot(4, 4, i+1)\n","    G = nx.from_numpy_array(gen_adj[i].numpy())\n","    G.remove_edges_from(nx.selfloop_edges(G))\n","    nx.draw_kamada_kawai(G, node_size=30)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gD6_EKoFYpNa"},"source":["Let us look at the graph embedding space."]},{"cell_type":"code","metadata":{"id":"MEd8_QEGT4MR"},"source":["batch = dgl.batch([graph for graph, label, full_adj in dataset])\n","y = np.array([label.item() for graph, label, full_adj in dataset])\n","with torch.no_grad():\n","    encoder.eval()\n","    z_mu, z_sigma = encoder(batch, batch.ndata['feat'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will visualize embeddings using TSNE decomposition."],"metadata":{"id":"9ymbNOenFdoc"}},{"cell_type":"code","metadata":{"id":"C9Tkp8_fVgh3"},"source":["reduction = TSNE(n_components=2)\n","xy_emb = reduction.fit_transform(z_mu)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W88zQQjsXA8n"},"source":["plt.figure(figsize=(10, 7))\n","for i, g_type in graph_type.items():\n","    plt.scatter(xy_emb[y==i, 0], xy_emb[y==i, 1], s=30, label=g_type)\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AlUJb8DmG-aJ"},"source":["### Graph GAN"]},{"cell_type":"markdown","source":["We consider the simplified Graph GAN model where we only generate the adjacency matrix."],"metadata":{"id":"5Q8yvdRbGWVb"}},{"cell_type":"markdown","metadata":{"id":"esGnh-odG38T"},"source":["by De Cao and Kipf [2018]"]},{"cell_type":"markdown","metadata":{"id":"nVZCLUU3Vz7e"},"source":["<img src='https://raw.githubusercontent.com/netspractice/advanced_gnn/made2021/lab_deep_generation/graph_gan.png' width=800>"]},{"cell_type":"markdown","source":["For simplicity, consider GAN trained on the truncated dataset. Select a subsample with lollipop graphs only."],"metadata":{"id":"3Gcd27DmGnmK"}},{"cell_type":"code","metadata":{"id":"Bzcs4Ge1kvNx"},"source":["max_num_v = 12\n","dataset = OrderingGCD(\n","    num_graphs=800*2,\n","    min_num_v=6,\n","    max_num_v=max_num_v,\n","    seed=0,\n","    force_reload=True\n",")\n","dataset = Subset(dataset, np.arange(600, 700))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e1RUqHSSk_bj"},"source":["N = len(dataset)\n","plt.figure(figsize=(10, 10))\n","np.random.seed(0)\n","for i in range(16):\n","    plt.subplot(4, 4, i+1)\n","    g, l, adj = dataset[np.random.randint(N)]\n","    g = nx.Graph(g.to_networkx())\n","    g.remove_edges_from(nx.selfloop_edges(g))\n","    nx.draw_kamada_kawai(g, node_size=30, node_color=[plt.cm.Set1.colors[l.item()]])\n","    plt.title(graph_type[l.item()])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define the dataloader."],"metadata":{"id":"Os0i1mhNGzsv"}},{"cell_type":"code","metadata":{"id":"MazBtkxTlVV_"},"source":["batch_size = 64\n","train_dataloader = DataLoader(\n","    dataset, batch_size=batch_size, drop_last=True, collate_fn=collate, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will use two-layer GCN with an average graph pooling as a discriminator. Since we will generate adjacency matrix, and then pass them through GCN, we cannot use message passing. Thus, implement GCN on matrix multiplication."],"metadata":{"id":"K5G988zOG22t"}},{"cell_type":"code","metadata":{"id":"bmxcHdX55Owu"},"source":["class PoolingGCN(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super().__init__()\n","        self.dense1 = nn.Linear(input_dim, hidden_dim)\n","        self.dense2 = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, adj, feat):\n","        norm = torch.diag_embed(adj.sum(dim=1)**-0.5)\n","        L = norm @ adj @ norm\n","        h = F.relu(L @ self.dense1(feat))\n","        h = self.dense2(h)\n","        return h.mean(dim=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sVOC9EbiSyRX"},"source":["disc = PoolingGCN(input_dim=max_num_v, hidden_dim=64, output_dim=1)\n","\n","adj = torch.rand(batch_size, 12, 12)\n","feat = torch.rand(batch_size, 12, max_num_v)\n","disc(adj, feat).shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will generate graph by two-layer MLP."],"metadata":{"id":"VtAcbZMCHglG"}},{"cell_type":"code","metadata":{"id":"cpGptfyfmErS"},"source":["class MLP(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, n_nodes):\n","        super().__init__()\n","        self.n_nodes = n_nodes\n","        self.MLP = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, n_nodes**2),\n","            nn.Sigmoid()\n","        )\n","    def forward(self, x):\n","        x = self.MLP(x)\n","        return x.reshape(-1, self.n_nodes, self.n_nodes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RYWwxKWNBmgq"},"source":["emb_dim = 32\n","gen = MLP(input_dim=emb_dim, hidden_dim=256, n_nodes=max_num_v)\n","\n","z = torch.randn(batch_size, emb_dim)\n","fake_adj = gen(z)\n","fake_adj.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XC-BMML-KzlB"},"source":["disc(fake_adj, feat).shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device"],"metadata":{"id":"W9WaSFbvcyCp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Training consists of two steps: generator optimization and k times of discriminator optimization. We also align graphs in a mini-batch by zero columns and rows so that graphs have adjacency matrices of the same size."],"metadata":{"id":"xS7s2V-IHpL0"}},{"cell_type":"code","metadata":{"id":"EFRat1rQUcYv"},"source":["def train(dataloader, gen, disc, gen_opt, disc_opt, batch_size, emb_dim):\n","    gen.train()\n","    disc.train()\n","    _gen_loss = []\n","    _disc_loss = []\n","    for real_graph, label, real_adj in dataloader:\n","\n","        features = []\n","        adjacency = []\n","        for g in dgl.unbatch(real_graph):\n","            k = max_num_v - g.number_of_nodes()\n","            features.append(F.pad(g.ndata['feat'], (0, 0, 0, k))[None, ...])\n","            adjacency.append(F.pad(g.adj().to_dense(), (0, k, 0, k))[None, ...])\n","        real_adj = torch.cat(adjacency)\n","        real_adj[:, range(max_num_v), range(max_num_v)] = 1\n","        real_feat = torch.cat(features)\n","\n","        real_feat = real_feat.to(device)\n","        real_adj = real_adj.to(device)\n","\n","        for _ in range(3):\n","            z = torch.randn(batch_size, emb_dim, device=device)\n","            with torch.no_grad():\n","                fake_adj = gen(z)\n","\n","            term1 = F.binary_cross_entropy_with_logits(\n","                disc(real_adj, real_feat), torch.ones(batch_size, 1, device=device))\n","            term2 = F.binary_cross_entropy_with_logits(\n","                disc(fake_adj, real_feat), torch.zeros(batch_size, 1, device=device))\n","            disc_loss = term1 + term2\n","\n","            disc_opt.zero_grad()\n","            disc_loss.backward()\n","            disc_opt.step()\n","\n","        z = torch.randn(batch_size, emb_dim, device=device)\n","        fake_adj = gen(z)\n","\n","        gen_loss = F.binary_cross_entropy_with_logits(\n","            disc(fake_adj, real_feat), torch.ones(batch_size, 1, device=device))\n","\n","        gen_opt.zero_grad()\n","        gen_loss.backward()\n","        gen_opt.step()\n","\n","        _disc_loss.append(disc_loss.item())\n","        _gen_loss.append(gen_loss.item())\n","\n","    return sum(_gen_loss)/len(_gen_loss), sum(_disc_loss)/len(_disc_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def discrete_adj(batch):\n","    adjs = []\n","    for pred_adj in batch:\n","        _adj = (pred_adj > 0.5).type(torch.LongTensor)\n","        _adj[range(max_num_v), range(max_num_v)] = 0\n","        mask = (_adj.sum(dim=1) != 0) | (_adj.sum(dim=0) != 0)\n","        adjs.append(_adj[mask, :][:, mask])\n","    return adjs"],"metadata":{"id":"RMRSausVY_Yt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define models and optimizers."],"metadata":{"id":"bqQVoi7mIzow"}},{"cell_type":"code","metadata":{"id":"uF1lt-zgucz7"},"source":["emb_dim = 32\n","gen = MLP(input_dim=emb_dim, hidden_dim=256, n_nodes=max_num_v)\n","disc = PoolingGCN(input_dim=4, hidden_dim=256, output_dim=1)\n","gen.to(device)\n","disc.to(device);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"giL0n3a_vUyN"},"source":["gen_opt = Adam(gen.parameters(), lr=0.005)\n","disc_opt = Adam(disc.parameters(), lr=0.005)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Train the models."],"metadata":{"id":"bRJEnNQQI2-m"}},{"cell_type":"code","metadata":{"id":"j4Gz4ndS3wI_"},"source":["n_epochs = 100\n","gen_loss = []\n","disc_loss = []\n","for epoch in range(n_epochs):\n","    g_loss, d_loss = train(train_dataloader, gen, disc, gen_opt, disc_opt, batch_size, emb_dim)\n","    gen_loss.append(g_loss)\n","    disc_loss.append(d_loss)\n","\n","    plt.figure(figsize=(12, 3))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(gen_loss[-200:], label='gen')\n","    plt.plot(disc_loss[-200:], label='disc')\n","    plt.legend()\n","    plt.title('GAN loss. Epoch: {}/{}'.format(epoch+1, n_epochs))\n","    plt.subplot(1, 2, 2)\n","    z = torch.randn(1, emb_dim, device=device)\n","    with torch.no_grad():\n","        fake_adj = gen(z).cpu()\n","    fake_adj = discrete_adj(fake_adj)\n","    G = nx.from_numpy_array(fake_adj[0].numpy())\n","    G.remove_edges_from(nx.selfloop_edges(G))\n","    nx.draw_kamada_kawai(G, node_size=30)\n","    plt.show()\n","    clear_output(wait=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UfadeQ9JZKqf"},"source":["Check the generation."]},{"cell_type":"code","metadata":{"id":"oIBJ7dnBbJky"},"source":["plt.figure(figsize=(10, 10))\n","z = torch.randn(batch_size, emb_dim)\n","gen.eval()\n","with torch.no_grad():\n","    fake_adj = gen(z)\n","fake_adj = discrete_adj(fake_adj)\n","for i in range(16):\n","    plt.subplot(4, 4, i+1)\n","    G = nx.from_numpy_array(fake_adj[i].numpy())\n","    G.remove_edges_from(nx.selfloop_edges(G))\n","    nx.draw_kamada_kawai(G, node_size=30)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5R31S8ny-FmH"},"execution_count":null,"outputs":[]}]}